exporters:
  otlp:
    endpoint: ${OTEL_ENVOY_ADDRESS}
    tls:
      insecure: ${OTEL_ENVOY_ADDRESS_TLS_INSECURE}
    headers:
      "Authorization": "Bearer ${SOLARWINDS_API_TOKEN}"
    retry_on_failure:
      enabled: {{ .Values.otel.metrics.retry_on_failure.enabled }}
      initial_interval: {{ .Values.otel.metrics.retry_on_failure.initial_interval }}
      max_interval: {{ .Values.otel.metrics.retry_on_failure.max_interval }}
      max_elapsed_time: {{ .Values.otel.metrics.retry_on_failure.max_elapsed_time }}
    sending_queue:
      enabled: {{ .Values.otel.metrics.sending_queue.enabled }}
      num_consumers: {{ .Values.otel.metrics.sending_queue.num_consumers }}
      queue_size: {{ .Values.otel.metrics.sending_queue.queue_size }}
{{- if .Values.otel.metrics.sending_queue.offload_to_disk }}
      storage: file_storage/sending_queue
{{- end }}
    timeout: {{ .Values.otel.metrics.timeout }}
extensions:
{{- if .Values.otel.metrics.sending_queue.offload_to_disk }}
  file_storage/sending_queue:
    directory: /var/lib/swo/sending_queue
{{- end }}
  health_check:
    endpoint: 0.0.0.0:13133

processors:
  k8sattributes:
{{ include "common.k8s-instrumentation" . | indent 4 }}
  memory_limiter:
{{ toYaml .Values.otel.metrics.memory_limiter | indent 4 }}
  transform:
    metric_statements:
      - context: datapoint
        statements:
          - set(attributes["job_condition"], "Active") where IsMatch(metric.name, "^.*kube_job_status_active$") == true and value_double > 0
          - set(attributes["job_condition"], "Failed") where IsMatch(metric.name, "^.*kube_job_failed$") == true and IsMatch(attributes["condition"], "^true$") == true and value_double > 0
          - set(attributes["job_condition"], "Complete") where IsMatch(metric.name, "^.*kube_job_complete$") == true and IsMatch(attributes["condition"], "^true$") == true and value_double > 0

  {{- include "common-config.filter-remove-internal" . | nindent 2 }}
  {{- include "common-config.filter-remove-internal-post-processing" . | nindent 2 }}

  transform/scope:
    metric_statements:
      - context: scope
        statements:
          - set(name, "")
          - set(version, "")
  # unify attributes
  attributes/unify_node_attribute:
    include:
      match_type: regexp
      metric_names:
        - container_.*
        - kube_node_.*
        - kube_pod_info
        - kube_pod_container_resource_requests
        - kube_pod_container_resource_limits
        - kube_pod_init_container_resource_requests
        - kube_pod_init_container_resource_limits
        - k8s.node_cpu_hourly_cost
        - k8s.node_gpu_hourly_cost
        - k8s.node_ram_hourly_cost
        - k8s.node_total_hourly_cost        
        - k8s.node_gpu_count
        - k8s.kubecost_node_is_spot                
    actions:
      - key: k8s.node.name
        from_attribute: node
        action: insert
{{- if not .Values.aws_fargate.enabled }}
      - key: k8s.node.name
        from_attribute: kubernetes_io_hostname
        action: insert
{{- end }}

  {{- include "common-config.transform-node-attributes" . | nindent 2 }}

  attributes/unify_volume_attribute:
    include:
      match_type: regexp
      metric_names:
        - kube_persistentvolumeclaim_.*
        - kube_persistentvolume_claim_ref
    actions:
      - key: persistentvolume
        from_attribute: volumename
        action: insert
      # for kube_persistentvolume_claim_ref
      - key: persistentvolumeclaim
        from_attribute: name
        action: insert
      - key: namespace
        from_attribute: claim_namespace
        action: upsert

  attributes/unify_service_attribute:
    include:
      match_type: regexp
      metric_names:
        - kube_service_.*
    actions:
      - key: k8s.service.name
        from_attribute: service
        action: insert
      - key: sw.k8s.service.external_name
        from_attribute: external_name
        action: insert
      - key: sw.k8s.service.type
        from_attribute: type
        action: insert
      - key: sw.k8s.cluster.ip
        from_attribute: cluster_ip
        action: insert

  attributes/unify_endpoint_attribute:
    include:
      match_type: regexp
      metric_names:
        - kube_endpoint_.*
    actions:
      - key: k8s.service.name
        from_attribute: endpoint
        action: insert
  attributes/unify_pod_attribute:
    include:
      match_type: regexp
      metric_names:
        - kube_pod_.*
    actions:
      - key: k8s.pod.uid
        from_attribute: uid
        action: insert

  attributes/identify_init_container:
    include:
      match_type: regexp
      metric_names:
        - kube_pod_init_container_.*
    actions:
      - key: sw.k8s.container.init
        action: insert
        value: "true"
  attributes/identify_standard_container:
    include:
      match_type: regexp
      metric_names:
        - kube_pod_container_.*
    actions:
      - key: sw.k8s.container.init
        action: insert
        value: "false"
  {{- include "common-config.attributes-remove-temp" . | nindent 2 }}

  metricstransform/rename:
    transforms:
      # add `k8s.` suffix to all metrics that are clearly provided by Kubernetes
      - include: ^(kube_|container_|kubernetes_|kubelet_|workqueue_|apiserver_|coredns_|etcd_|certmanager_)(.*)$$
        match_type: regexp
        action: update
        new_name: k8s.$${1}$${2}
  
  metricstransform/rename-otel:
    transforms:
      # add `k8s.` prefix to all metrics
      - include: ^(.*)$$
        match_type: regexp
        action: update
        new_name: k8s.$${1}

  # Transformations done on all metrics before any grouping
  metricstransform/preprocessing:
    transforms:
      - include: k8s.kube_node_status_condition
        experimental_match_labels: { "condition": "Ready" }
        action: insert
        new_name: k8s.kube_node_status_ready
        operations:
          - action: update_label
            label: status
            value_actions:
              - value: "true"
                new_value: "Ready"
              - value: "false"
                new_value: "NotReady"
              - value: unknown
                new_value: Unknown
          - action: update_label
            label: status
            new_label: sw.k8s.node.status
      - include: k8s.kube_deployment_status_condition
        experimental_match_labels: { "condition": "Available" }
        action: insert
        new_name: k8s.deployment.condition.available
        operations:
          - action: update_label
            label: status
            new_label: sw.k8s.deployment.condition.available
      - include: k8s.kube_deployment_status_condition
        experimental_match_labels: { "condition": "Progressing" }
        action: insert
        new_name: k8s.deployment.condition.progressing
        operations:
          - action: update_label
            label: status
            new_label: sw.k8s.deployment.condition.progressing
      - include: k8s.kube_deployment_status_condition
        experimental_match_labels: { "condition": "ReplicaFailure" }
        action: insert
        new_name: k8s.deployment.condition.replicafailure
        operations:
          - action: update_label
            label: status
            new_label: sw.k8s.deployment.condition.replicafailure
      - include: k8s.kube_persistentvolume_status_phase
        action: insert
        new_name: k8s.persistentvolume.status.phase
        operations:
          - action: update_label
            label: phase
            new_label: sw.k8s.persistentvolume.status
      - include: k8s.kube_persistentvolumeclaim_status_phase
        action: insert
        new_name: k8s.persistentvolumeclaim.status.phase
        operations:
          - action: update_label
            label: phase
            new_label: sw.k8s.persistentvolumeclaim.status

      {{- include "common-config.metricstransform-preprocessing-cadvisor" . | nindent 6 }}
      # Container metrics
      - include: k8s.kube_pod_container_resource_limits
        experimental_match_labels: { "resource": "cpu" }
        action: insert
        new_name: k8s.container.spec.cpu.limit_temp
      - include: k8s.kube_pod_init_container_resource_limits
        experimental_match_labels: { "resource": "cpu" }
        action: insert
        new_name: k8s.initcontainer.spec.cpu.limit_temp

      - include: k8s.kube_pod_container_resource_requests
        experimental_match_labels: { "resource": "cpu" }
        action: insert
        new_name: k8s.container.spec.cpu.requests_temp
      - include: k8s.kube_pod_init_container_resource_requests
        experimental_match_labels: { "resource": "cpu" }
        action: insert
        new_name: k8s.initcontainer.spec.cpu.requests_temp
      - include: (k8s.container.spec.cpu.requests_temp|k8s.initcontainer.spec.cpu.requests_temp)
        match_type: regexp
        action: combine
        new_name: k8s.container.spec.cpu.requests

      - include: k8s.kube_pod_container_resource_requests
        experimental_match_labels: { "resource": "memory" }
        action: insert
        new_name: k8s.container.spec.memory.requests_temp
      - include: k8s.kube_pod_init_container_resource_requests
        experimental_match_labels: { "resource": "memory" }
        action: insert
        new_name: k8s.initcontainer.spec.memory.requests_temp
      - include: (k8s.container.spec.memory.requests_temp|k8s.initcontainer.spec.memory.requests_temp)
        match_type: regexp
        action: combine
        new_name: k8s.container.spec.memory.requests

      - include: k8s.kube_pod_container_resource_limits
        action: insert
        match_type: regexp
        experimental_match_labels: { "resource": "memory" }
        new_name: k8s.container.spec.memory.limit_temp
      - include: k8s.kube_pod_init_container_resource_limits
        action: insert
        match_type: regexp
        experimental_match_labels: { "resource": "memory" }
        new_name: k8s.initcontainer.spec.memory.limit_temp

      - include: k8s.kube_pod_container_status_waiting
        action: insert
        new_name: k8s.kube_pod_container_status_waiting_only_temp
      - include: k8s.kube_pod_container_status_running
        action: insert
        new_name: k8s.kube_pod_container_status_running_only_temp
      - include: k8s.kube_pod_container_status_terminated
        action: insert
        new_name: k8s.kube_pod_container_status_terminated_only_temp
      - include: ^k8s.kube_pod_container_status_(?P<status>[^_]*)_only_temp$
        match_type: regexp
        action: combine
        new_name: k8s.container.status_temp
        submatch_case: lower
        operations:
          - action: update_label
            label: status
            new_label: sw.k8s.container.status
      - include: k8s.kube_pod_init_container_status_waiting
        action: insert
        new_name: k8s.kube_pod_init_container_status_waiting_only_temp
      - include: k8s.kube_pod_init_container_status_running
        action: insert
        new_name: k8s.kube_pod_init_container_status_running_only_temp
      - include: k8s.kube_pod_init_container_status_terminated
        action: insert
        new_name: k8s.kube_pod_init_container_status_terminated_only_temp
      - include: ^k8s.kube_pod_init_container_status_(?P<status>[^_]*)_only_temp$
        match_type: regexp
        action: combine
        new_name: k8s.initcontainer.status_temp
        submatch_case: lower
        operations:
          - action: update_label
            label: status
            new_label: sw.k8s.container.status
      - include: (k8s.initcontainer.status_temp|k8s.container.status_temp)
        match_type: regexp
        action: combine
        submatch_case: lower
        new_name: k8s.container.status

      # Pod resource metrics
      - include: k8s.container.spec.cpu.limit_temp
        action: insert
        new_name: k8s.pod.spec.cpu.limit
        operations:
          - action: aggregate_labels
            label_set: [pod, namespace, k8s.node.name]
            aggregation_type: sum
      - include: (k8s.container.spec.cpu.limit_temp|k8s.initcontainer.spec.cpu.limit_temp)
        match_type: regexp
        action: combine
        new_name: k8s.container.spec.cpu.limit
      - include: k8s.container.spec.memory.limit_temp
        action: insert
        new_name: k8s.pod.spec.memory.limit
        operations:
          - action: aggregate_labels
            label_set: [pod, namespace, k8s.node.name]
            aggregation_type: sum
      - include: (k8s.container.spec.memory.limit_temp|k8s.initcontainer.spec.memory.limit_temp)
        match_type: regexp
        action: combine
        new_name: k8s.container.spec.memory.limit

      - include: k8s.kube_pod_container_status_running
        action: insert
        new_name: k8s.pod.containers.running
        operations:
          - action: aggregate_labels
            label_set: [pod, namespace]
            aggregation_type: sum
      - include: k8s.container.spec.cpu.requests
        action: insert
        new_name: k8s.pod.spec.cpu.requests
        operations:
          - action: aggregate_labels
            label_set: [pod, namespace, k8s.node.name]
            aggregation_type: sum
      - include: k8s.container.spec.memory.requests
        action: insert
        new_name: k8s.pod.spec.memory.requests
        operations:
          - action: aggregate_labels
            label_set: [pod, namespace, k8s.node.name]
            aggregation_type: sum
      - include: k8s.kube_pod_owner
        experimental_match_labels: { "owner_kind": "DaemonSet", "owner_is_controller": "true" }
        action: insert
        new_name: k8s.kube.pod.owner.daemonset
        operations:
          - action: update_label
            label: owner_name
            new_label: daemonset
      - include: k8s.kube_pod_owner
        experimental_match_labels: { "owner_kind": "ReplicaSet", "owner_is_controller": "true" }
        action: insert
        new_name: k8s.kube.pod.owner.replicaset
        operations:
          - action: update_label
            label: owner_name
            new_label: replicaset
      - include: k8s.kube_pod_owner
        experimental_match_labels: { "owner_kind": "StatefulSet", "owner_is_controller": "true" }
        action: insert
        new_name: k8s.kube.pod.owner.statefulset
        operations:
          - action: update_label
            label: owner_name
            new_label: statefulset
      - include: k8s.kube_pod_owner
        experimental_match_labels: { "owner_kind": "Job", "owner_is_controller": "true" }
        action: insert
        new_name: k8s.kube.pod.owner.job
        operations:
          - action: update_label
            label: owner_name
            new_label: job_name
      - include: k8s.kube_replicaset_owner
        experimental_match_labels: { "owner_kind": "Deployment", "owner_is_controller": "true" }
        action: insert
        new_name: k8s.kube.replicaset.owner.deployment
        operations:
          - action: update_label
            label: owner_name
            new_label: deployment
      - include: k8s.kube_node_status_capacity
        experimental_match_labels: { "resource": "cpu" }
        action: insert
        new_name: k8s.node.cpu.capacity
      - include: k8s.kube_node_status_allocatable
        experimental_match_labels: { "resource": "cpu" }
        action: insert
        new_name: k8s.node.cpu.allocatable
      - include: k8s.kube_node_status_capacity
        experimental_match_labels: { "resource": "memory" }
        action: insert
        new_name: k8s.node.memory.capacity
      - include: k8s.kube_node_status_allocatable
        experimental_match_labels: { "resource": "memory" }
        action: insert
        new_name: k8s.node.memory.allocatable
      - include: k8s.kube_node_status_condition
        experimental_match_labels: { "condition": "Ready", "status": "true" }
        action: insert
        new_name: k8s.node.status.condition.ready
      - include: k8s.kube_node_status_condition
        experimental_match_labels: { "condition": "NetworkUnavailable", "status": "true" }
        action: insert
        new_name: k8s.node.status.condition.networkunavailable
      - include: k8s.kube_node_status_condition
        experimental_match_labels: { "condition": "PIDPressure", "status": "true" }
        action: insert
        new_name: k8s.node.status.condition.pidpressure
      - include: k8s.kube_node_status_condition
        experimental_match_labels: { "condition": "MemoryPressure", "status": "true" }
        action: insert
        new_name: k8s.node.status.condition.memorypressure
      - include: k8s.kube_node_status_condition
        experimental_match_labels: { "condition": "DiskPressure", "status": "true" }
        action: insert
        new_name: k8s.node.status.condition.diskpressure
      - include: k8s.kube_pod_status_phase
        experimental_match_labels: { "phase": "Running" }
        action: insert
        new_name: k8s.pod.status.phase.running_temp
      
      # Cluster metrics
      - include: k8s.kube_pod_info
        action: insert
        new_name: k8s.cluster.pods
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.kube_node_info
        action: insert
        new_name: k8s.cluster.nodes
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.node.status.condition.ready
        action: insert
        new_name: k8s.cluster.nodes.ready
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.node.status.condition.ready
        action: insert
        new_name: k8s.cluster.nodes.ready.avg
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: mean
      - include: k8s.container.spec.memory.requests
        action: insert
        new_name: k8s.cluster.spec.memory.requests
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.container.spec.cpu.requests
        action: insert
        new_name: k8s.cluster.spec.cpu.requests
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.pod.status.phase.running_temp
        action: insert
        new_name: k8s.cluster.pods.running
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.node.cpu.capacity
        action: insert
        new_name: k8s.cluster.cpu.capacity
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.node.cpu.allocatable
        action: insert
        new_name: k8s.cluster.cpu.allocatable
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.node.memory.capacity
        action: insert
        new_name: k8s.cluster.memory.capacity
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.node.memory.allocatable
        action: insert
        new_name: k8s.cluster.memory.allocatable
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.kubernetes_build_info
        action: insert
        match_type: regexp
        experimental_match_labels: { "scrape_job": '.*apiservers.*' }
        new_name: k8s.cluster.version
        operations:
          - action: update_label
            label: git_version
            new_label: sw.k8s.cluster.version
          - action: aggregate_labels
            label_set: [sw.k8s.cluster.version]
            aggregation_type: sum
      - include: k8s.kubernetes_build_info
        action: update
        new_name: k8s.kubernetes_build_info_temp
      # Prometheus metrics
      - include: k8s.apiserver_request_total
        action: insert
        match_type: regexp
        # Alternative to (?!5\d\d|429) - Go regex does not support negative lookahead
        experimental_match_labels: { "code": '^(([0-3]|[6-9])\d\d)|(4([0-1]|[3-9])\d)|(42[0-8])$' }
        new_name: apiserver_request_not_failed_temp
      - include: apiserver_request_not_failed_temp
        action: update
        new_name: apiserver_request_not_failed_temp
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
      - include: k8s.apiserver_request_total
        action: insert
        new_name: apiserver_request_total_temp
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum

      # Job metrics
      - include: k8s.kube_job_owner
        experimental_match_labels: { "owner_kind": "CronJob", "owner_is_controller": "true" }
        action: insert
        new_name: k8s.kube.job.owner.cronjob
        operations:
          - action: update_label
            label: owner_name
            new_label: cronjob

  filter/preprocessing:
    error_mode: ignore
    metrics:
      datapoint:
        - 'metric.name == "k8s.kube_node_status_ready" and value_double != 1'
        - 'metric.name == "k8s.deployment.condition.available" and value_double != 1'
        - 'metric.name == "k8s.deployment.condition.progressing" and value_double != 1'
        - 'metric.name == "k8s.deployment.condition.replicafailure" and value_double != 1'
        - 'metric.name == "k8s.pod.status.reason" and value_double != 1'
        - 'metric.name == "k8s.kube_pod_status_phase" and value_double != 1'
        - 'metric.name == "k8s.kube_pod_start_time" and value_double == 0'
        - 'metric.name == "k8s.kube_pod_completion_time" and value_double == 0'
        - 'metric.name == "k8s.persistentvolume.status.phase" and value_double != 1'
        - 'metric.name == "k8s.persistentvolumeclaim.status.phase" and value_double != 1'
        - 'metric.name == "k8s.kube_node_created" and value_double == 0'
        - 'metric.name == "k8s.kube_pod_created" and value_double == 0'
        - 'metric.name == "k8s.kube_deployment_created" and value_double == 0'
        - 'metric.name == "k8s.kube_daemonset_created" and value_double == 0'
        - 'metric.name == "k8s.kube_namespace_status_phase" and value_double != 1'
        - 'metric.name == "k8s.kube_namespace_created" and value_double == 0'
        - 'metric.name == "k8s.kube_statefulset_created" and value_double == 0'
        - 'metric.name == "k8s.kube_job_created" and value_double == 0'
        - 'metric.name == "k8s.kube_job_status_completion_time" and value_double == 0'
        - 'metric.name == "k8s.kube_job_status_start_time" and value_double == 0'
        - 'metric.name == "k8s.container.status" and value_double != 1'

  cumulativetodelta:
    include:
      metrics:
        {{- include "common-config.cumulativetorate-cadvisor" . | nindent 8 }}
        - apiserver_request_not_failed_temp
        - apiserver_request_total_temp
      match_type: strict

  deltatorate:
    metrics:
      {{- include "common-config.cumulativetorate-cadvisor" . | nindent 6 }}
  
  metricsgeneration/cluster:
    rules:
      - name: k8s.apiserver.request.successrate
        unit: Percent
        type: calculate
        metric1: apiserver_request_not_failed_temp
        metric2: apiserver_request_total_temp
        operation: percent
  
  groupbyattrs/node:
    keys:
      - k8s.node.name
  # Transformations done after grouping per k8s.node.name
  metricstransform/aggregate_node_level:
    transforms:
      - include: k8s.kube_pod_info
        action: insert
        new_name: k8s.node.pods
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
  groupbyattrs/pod:
    keys:
      - namespace
      - pod
  # Transformations done after grouping per pod
  metricstransform/aggregate_pod_level:
    transforms:
      - include: k8s.kube_pod_container_info
        action: insert
        new_name: k8s.pod.containers
        operations:
          - action: aggregate_labels
            # use [dummy_label_workaround] instead of [] as a workaround for a bug introduced in metricstransform 0.106
            label_set: [dummy_label_workaround]
            aggregation_type: sum
  groupbyattrs/all:
    keys:
      - kubelet_version
      - container_runtime_version
      - provider_id
      - os_image
      - namespace
      - uid
      - k8s.pod.uid
      - pod_ip
      - host_ip
      - created_by_kind
      - created_by_name
      - host_network
      - priority_class
      - container_id
      - container
      - image
      - image_id
      - k8s.node.name
      - sw.k8s.namespace.status
      - sw.k8s.node.status
      - sw.k8s.container.status
      - sw.k8s.container.init
      - daemonset
      - statefulset
      - deployment
      - replicaset
      - job_name
      - cronjob
      - sw.k8s.cluster.version
      - internal_ip
      - job_condition
      - persistentvolumeclaim
      - persistentvolume
      - sw.k8s.persistentvolumeclaim.status
      - sw.k8s.persistentvolume.status
      - storageclass
      - access_mode
      - k8s.service.name
      - sw.k8s.service.external_name
      - sw.k8s.service.type
      - sw.k8s.cluster.ip
{{- if .Values.otel.metrics.filter }}
  filter:
    metrics:
{{ toYaml .Values.otel.metrics.filter | indent 6 }}
{{- end }}

{{- include "common-config.filter-remove-temporary-metrics" . | nindent 2 }}

{{- include "common-config.filter-reciever" . | nindent 2 }}

  attributes/attributes_namespace_status:
    include:
      match_type: regexp
      metric_names:
        - k8s.kube_namespace_status_phase
    actions:
      - key: sw.k8s.namespace.status
        from_attribute: phase
        action: insert

  {{- include "common-config.resource-metrics" . | nindent 2 }}

  transform/cleanup_attributes_for_nonexisting_entities:
    metric_statements:
      - context: metric
        statements:
         # do not create/update node entities from metrics
          - set(resource.attributes["sw.entity.noupdate"], "true") where resource.attributes["k8s.node.name"] != nil

  attributes/remove_prometheus_attributes_endpoint:
    exclude:
      match_type: regexp
      metric_names:
        - kube_endpoint_.*
    actions:
      - key: endpoint
        action: delete

  {{- include "common-config.attributes-remove-prometheus-attributes" . | nindent 2 }}

  resource/otlp-metrics:
    attributes:
      # Collector and Manifest version
      - key: sw.k8s.agent.manifest.version
        value: ${MANIFEST_VERSION}
        action: insert

      - key: sw.k8s.agent.app.version
        value: ${APP_VERSION}
        action: insert

      # Cluster
      - key: sw.k8s.cluster.uid
        value: ${CLUSTER_UID}
        action: insert

      - key: k8s.cluster.name
        value: ${CLUSTER_NAME}
        action: insert

  batch:
{{ toYaml .Values.otel.metrics.batch | indent 4 }}
  filter/kube-state-metrics:
    metrics:
      metric:
        - |
          not(
          name == "kube_deployment_created" or
          name == "kube_deployment_created" or
          name == "kube_daemonset_created" or
          name == "kube_namespace_created" or
          name == "kube_node_info" or
          name == "kube_node_created" or
          name == "kube_node_status_capacity" or
          name == "kube_node_status_condition" or
          name == "kube_pod_created" or
          name == "kube_pod_info" or
          name == "kube_pod_owner" or
          name == "kube_pod_completion_time" or
          name == "kube_pod_status_phase" or
          name == "kube_pod_status_ready" or
          name == "kube_pod_status_reason" or
          name == "kube_pod_start_time" or 
          IsMatch(name, "^kube_pod_container_.*$") or 
          IsMatch(name, "^kube_pod_init_container_.*$") or 
          name == "kube_namespace_status_phase" or
          name == "kube_deployment_spec_replicas" or
          name == "kube_deployment_spec_paused" or
          name == "kube_deployment_status_replicas" or
          name == "kube_deployment_status_replicas_ready" or
          name == "kube_deployment_status_replicas_available" or
          name == "kube_deployment_status_replicas_updated" or
          name == "kube_deployment_status_replicas_unavailable" or
          name == "kube_deployment_status_condition" or
          name == "kube_replicaset_owner" or
          name == "kube_replicaset_created" or
          name == "kube_replicaset_spec_replicas" or
          name == "kube_replicaset_status_ready_replicas" or
          name == "kube_replicaset_status_replicas" or
          name == "kube_statefulset_replicas" or
          name == "kube_statefulset_status_replicas_ready" or
          name == "kube_statefulset_status_replicas_current" or
          name == "kube_statefulset_status_replicas_updated" or
          name == "kube_statefulset_created" or
          name == "kube_daemonset_status_current_number_scheduled" or
          name == "kube_daemonset_status_desired_number_scheduled" or
          name == "kube_daemonset_status_updated_number_scheduled" or
          name == "kube_daemonset_status_number_available" or
          name == "kube_daemonset_status_number_misscheduled" or
          name == "kube_daemonset_status_number_ready" or
          name == "kube_daemonset_status_number_unavailable" or
          name == "kube_resourcequota" or
          name == "kube_node_status_allocatable" or
          name == "kube_node_spec_unschedulable" or
          name == "kube_job_info" or
          name == "kube_job_owner" or
          name == "kube_job_created" or
          name == "kube_job_complete" or
          name == "kube_job_failed" or
          name == "kube_job_status_active" or
          name == "kube_job_status_succeeded" or
          name == "kube_job_status_failed" or
          name == "kube_job_status_start_time" or
          name == "kube_job_status_completion_time" or
          name == "kube_job_spec_completions" or
          name == "kube_job_spec_parallelism" or
          name == "kube_persistentvolume_capacity_bytes" or
          name == "kube_persistentvolume_info" or
          name == "kube_persistentvolume_status_phase" or
          name == "kube_persistentvolume_claim_ref" or
          name == "kube_persistentvolume_created" or
          name == "kube_persistentvolumeclaim_info" or
          name == "kube_persistentvolumeclaim_access_mode" or
          name == "kube_persistentvolumeclaim_status_phase" or
          name == "kube_persistentvolumeclaim_resource_requests_storage_bytes" or
          name == "kube_persistentvolumeclaim_created" or
          name == "kube_pod_spec_volumes_persistentvolumeclaims_info" or
          name == "kube_service_info" or
          name == "kube_service_created" or
          name == "kube_service_spec_type" or
          name == "kube_service_spec_external_ip" or
          name == "kube_service_status_load_balancer_ingress" or
          name == "kube_endpoint_info" or
          name == "kube_endpoint_created" or
          name == "kube_endpoint_ports" or
          name == "kube_endpoint_address"
          )
  filter/ebpf:
    metrics:
      metric:
        - 'IsMatch(name, "^ebpf_net.*$")'

  filter/prometheus-node-metrics:
    metrics:
      metric:
        - |
          not(
          name == "container_cpu_usage_seconds_total" or
          name == "container_spec_cpu_quota" or
          name == "container_spec_cpu_period" or
          name == "container_memory_working_set_bytes" or
          name == "container_spec_memory_limit_bytes" or
          name == "container_cpu_cfs_throttled_periods_total" or
          name == "container_cpu_cfs_periods_total" or
          name == "container_fs_reads_total" or
          name == "container_fs_writes_total" or
          name == "container_fs_reads_bytes_total" or
          name == "container_fs_writes_bytes_total" or
          name == "container_fs_usage_bytes" or
          name == "container_network_receive_bytes_total" or
          name == "container_network_transmit_bytes_total" or
          name == "container_network_receive_packets_total" or
          name == "container_network_transmit_packets_total" or
          name == "container_network_receive_packets_dropped_total" or
          name == "container_network_transmit_packets_dropped_total" or
          name == "apiserver_request_total" or
          name == "apiserver_request_duration_seconds" or
          name == "workqueue_adds_total" or
          name == "workqueue_depth" or
          name == "workqueue_queue_duration_seconds" or
          name == "kubelet_volume_stats_available_percent" or
          name == "kubernetes_build_info" or
          name == "coredns_build_info" or
          name == "coredns_dns_requests_total" or
          name == "coredns_dns_request_size_bytes" or
          name == "coredns_dns_responses_total" or
          name == "coredns_dns_response_size_bytes" or
          name == "coredns_dns_request_duration_seconds" or
          name == "coredns_cache_entries" or
          name == "coredns_cache_hits_total" or
          name == "coredns_cache_misses_total" or
          name == "certmanager_certificate_expiration_timestamp_seconds" or
          name == "certmanager_certificate_ready_status" or
          name == "certmanager_certificate_renewal_timestamp_seconds" or
          name == "certmanager_clock_time_seconds" or
          name == "certmanager_clock_time_seconds_gauge" or
          name == "certmanager_controller_sync_call_count" or
          name == "certmanager_http_acme_client_request_count"
          )
  filter/histograms:
    metrics:
      metric:
        - 'type == METRIC_DATA_TYPE_HISTOGRAM and not(name == "k8s.apiserver_request_duration_seconds" or name == "k8s.workqueue_queue_duration_seconds" or name == "k8s.coredns_dns_request_duration_seconds" or name == "k8s.coredns_dns_request_size_bytes" or name == "k8s.coredns_dns_response_size_bytes")'

connectors:
  forward/prometheus:
  forward/metric-exporter:

receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:{{ .Values.otel.metrics.otlp_endpoint.port }}
{{- if and (and .Values.otel.metrics.extra_scrape_metrics (or (not .Values.otel.metrics.autodiscovery.prometheusEndpoints.enabled) .Values.otel.metrics.force_extra_scrape_metrics)) .Values.otel.metrics.prometheus.url }}
  prometheus/prometheus-server:
    config:
      scrape_configs:
        - job_name: prometheus
          scheme: {{ .Values.otel.metrics.prometheus.scheme | default "http" | quote }}
          scrape_interval: {{ quote .Values.otel.metrics.prometheus.scrape_interval }}
          metrics_path: "/federate"
          honor_timestamps: false
          honor_labels: true
          params:
            "match[]":
{{ toYaml .Values.otel.metrics.extra_scrape_metrics | indent 14 }}
          static_configs:
            - targets:
                - ${PROMETHEUS_URL}
{{- end }}
  prometheus/kube-state-metrics:
    config:
      scrape_configs:
        - job_name: kube-state-metrics
          scheme: {{ (index .Values.otel.metrics "kube-state-metrics").scheme | default "http" | quote }}
          scrape_interval: {{ quote (index .Values.otel.metrics "kube-state-metrics").scrape_interval }}
          metrics_path: "/metrics"
          honor_timestamps: false
          honor_labels: true
          static_configs:
            - targets:
                - ${KUBE_STATE_METRICS_URL}
  prometheus/node-metrics:
    config:
      scrape_configs:
{{- if .Values.aws_fargate.enabled }}
        - job_name: 'kubernetes-nodes-cadvisor'
          honor_timestamps: false
          scrape_interval: {{ quote .Values.otel.metrics.prometheus.scrape_interval }}
          scrape_timeout: 10s
          metrics_path: /metrics
          scheme: https
          authorization:
            type: Bearer
            credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          follow_redirects: true
          enable_http2: true
          relabel_configs:
          - separator: ;
            regex: __meta_kubernetes_node_label_(.+)
            replacement: $$1
            action: labelmap
          - separator: ;
            regex: (.*)
            target_label: __address__
            replacement: kubernetes.default.svc:443
            action: replace
          - source_labels: [__meta_kubernetes_node_name]
            separator: ;
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$$1/proxy/metrics/cadvisor
            action: replace
          # manually label job source as original `job` label can be misleading
          - source_labels: [__address__]
            target_label: scrape_job
            replacement: "kubernetes-nodes-cadvisor"
          kubernetes_sd_configs:
          - role: node
            kubeconfig_file: ""
            follow_redirects: true
            enable_http2: true
        - job_name: 'kubernetes-nodes'
          honor_timestamps: false
          scrape_interval: {{ quote .Values.otel.metrics.prometheus.scrape_interval }}
          scrape_timeout: 10s
          metrics_path: /metrics
          scheme: https
          authorization:
            type: Bearer
            credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          follow_redirects: true
          enable_http2: true
          relabel_configs:
          - separator: ;
            regex: __meta_kubernetes_node_label_(.+)
            replacement: $$1
            action: labelmap
          - separator: ;
            regex: (.*)
            target_label: __address__
            replacement: kubernetes.default.svc:443
            action: replace
          - source_labels: [__meta_kubernetes_node_name]
            separator: ;
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/$$1/proxy/metrics
            action: replace
          # manually label job source as original `job` label can be misleading
          - source_labels: [__address__]
            target_label: scrape_job
            replacement: "kubernetes-nodes"
          kubernetes_sd_configs:
          - role: node
            kubeconfig_file: ""
            follow_redirects: true
            enable_http2: true
{{- end }}
        - job_name: kubernetes-apiservers
          honor_timestamps: false
          scrape_interval: {{ quote .Values.otel.metrics.prometheus.scrape_interval }}
          scrape_timeout: 10s
          metrics_path: /metrics
          scheme: https
          authorization:
            type: Bearer
            credentials_file: /var/run/secrets/kubernetes.io/serviceaccount/token
          tls_config:
            ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
            insecure_skip_verify: true
          follow_redirects: true
          enable_http2: true
          relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            separator: ;
            regex: default;kubernetes;https
            replacement: $$1
            action: keep
          # manually label job source as original `job` label can be misleading
          - source_labels: [__address__]
            target_label: scrape_job
            replacement: "kubernetes-apiservers"
          kubernetes_sd_configs:
          - role: endpoints
            kubeconfig_file: ""
            follow_redirects: true
            enable_http2: true
        {{- if .Values.otel.metrics.control_plane.coredns.enabled }}
        - job_name: kubernetes-coredns
          honor_timestamps: false
          scrape_interval: {{ quote .Values.otel.metrics.prometheus.scrape_interval }}
          scrape_timeout: 10s
          kubernetes_sd_configs:
          - role: endpoints
            follow_redirects: true
            enable_http2: true
          relabel_configs:
          - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: kube-dns;kube-system;metrics  # Only scrape CoreDNS in kube-system namespace
          # manually label job source as original `job` label can be misleading
          - source_labels: [__address__]
            target_label: scrape_job
            replacement: "kubernetes-coredns"
        {{- end }}
        {{- if and .Values.otel.metrics.control_plane.etcd.enabled (eq .Values.otel.metrics.control_plane.etcd.scrape_kind "static") }}
        - job_name: kubernetes-etcd
          scheme: {{ quote .Values.otel.metrics.control_plane.etcd.scheme }}
          scrape_interval: {{ quote .Values.otel.metrics.prometheus.scrape_interval }}
          metrics_path: {{ quote .Values.otel.metrics.control_plane.etcd.metrics_path }}
          honor_timestamps: false
          honor_labels: true
          static_configs:
            - targets:
                {{- range $endpoint := .Values.otel.metrics.control_plane.etcd.static_endpoints }}
                - {{ $endpoint}}
                {{- end }}
        {{- end }}
        - job_name: cert-manager
          honor_timestamps: false
          scrape_interval: {{ quote .Values.otel.metrics.prometheus.scrape_interval }}
          scrape_timeout: 10s
          kubernetes_sd_configs:
          - role: endpoints
            follow_redirects: true
            enable_http2: true
          relabel_configs:
          - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_instance, __meta_kubernetes_service_label_app_kubernetes_io_name]
            regex: cert-manager
            action: keep

service:
  extensions:
{{- if .Values.otel.metrics.sending_queue.offload_to_disk }}
    - file_storage/sending_queue
{{- end }}
    - health_check
  pipelines:
    metrics/otlp:
      exporters:
        - forward/metric-exporter
      processors:
        - memory_limiter
{{- if and .Values.ebpfNetworkMonitoring.enabled (not .Values.ebpfNetworkMonitoring.reducer.telemetry.metrics.enabled)  }}
        - filter/ebpf
{{- end }}
        - metricstransform/rename-otel
        - resource/otlp-metrics
      receivers:
        - otlp
{{- if and (and .Values.otel.metrics.extra_scrape_metrics (or (not .Values.otel.metrics.autodiscovery.prometheusEndpoints.enabled) .Values.otel.metrics.force_extra_scrape_metrics)) .Values.otel.metrics.prometheus.url }}
    metrics/prometheus-server:
      exporters:
        - forward/prometheus
      processors:
        - memory_limiter
      receivers:
        - prometheus/prometheus-server
{{- end }}
    metrics/kubestatemetrics:
      exporters:
        - forward/prometheus
      processors:
        - memory_limiter
        - filter/kube-state-metrics
      receivers:
        - prometheus/kube-state-metrics
    metrics/prometheus-node-metrics:
      exporters:
        - forward/prometheus
      processors:
        - memory_limiter
        - filter/prometheus-node-metrics
      receivers:
        - prometheus/node-metrics
    metrics/prometheus:
      exporters:
        - forward/metric-exporter
      processors:
        - memory_limiter
        - filter/receiver
        - transform
        - filter/remove_internal        
        - attributes/remove_prometheus_attributes
        - attributes/remove_prometheus_attributes_endpoint
        - attributes/unify_node_attribute
        - transform/unify_node_attribute
        - attributes/unify_volume_attribute        
        - attributes/unify_service_attribute
        - attributes/unify_endpoint_attribute
        - attributes/unify_pod_attribute
        - attributes/identify_init_container
        - attributes/identify_standard_container
        - metricstransform/rename
        - metricstransform/preprocessing
        - filter/preprocessing
        - filter/remove_internal_postprocessing
        - attributes/remove_temp
        - attributes/attributes_namespace_status
        - cumulativetodelta
        - deltatorate
        - metricsgeneration/cluster
        - groupbyattrs/node
        - metricstransform/aggregate_node_level
        - groupbyattrs/pod
        - metricstransform/aggregate_pod_level
        - groupbyattrs/all
        - resource/metrics
        - k8sattributes
        - transform/cleanup_attributes_for_nonexisting_entities
{{- if .Values.otel.metrics.filter }}
        - filter
{{- end }}
        - filter/remove_temporary_metrics
      receivers:
        - forward/prometheus
    metrics:
      exporters:
        - otlp
      processors:
        - memory_limiter
        - filter/histograms
        - transform/scope
        - batch
      receivers:
        - forward/metric-exporter
    
  telemetry:
{{- if .Values.otel.metrics.telemetry.logs.enabled }}
    logs:
      level: {{ .Values.otel.metrics.telemetry.logs.level }}
{{- end }}
{{- if .Values.otel.metrics.telemetry.metrics.enabled }}
    metrics:
      address: {{ .Values.otel.metrics.telemetry.metrics.address }}
{{- end }}
